# Starter pipeline
# Start with a minimal pipeline that you can customize to build and deploy your code.
# Add steps that build, run tests, deploy, and more:
# https://aka.ms/yaml

variables:
  scheduler_pod: 'airflow-helm-scheduler-767576bb4c-r2plw -c scheduler'
  NAMESPACE_HELM: airflow-helm
  NAMESPACE_STACK: airflow-stackable
  SCHED_POD_CORE_HELM: airflow-helm-dev-scheduler-
  SCHED_POD_CORE_STACK: airflow-scheduler-default-0
  AIRFLOW_HOME_HELM: opt
  AIRFLOW_HOME_STACK: stackable
  CONTAINER_HELM: scheduler
  CONTAINER_SCHED: airflow

  KUBECONFIG: config.yaml
  OCP_LOGIN_URL: https://api.crc.testing:6443
  OCP_BASE_DOMAIN: apps-crc.testing

  CIM_GROUP: ansr
  
  #CIM_APP_PROJECT: ${APP_NAME}
  #DEVOPS_REGISTRY: ${CI_REGISTRY}/${CIM_GROUP}/nrs-ds-ci
  KUBECTL_TEKTON_IMAGE: kubectl-tekton

  CI_REGISTRY_USER: ${GITLAB_USER}
  CI_REGISTRY_PASSWORD: ${GITLAB_PASSWORD}


trigger:
- main

pool:
  vmImage: ubuntu-latest

steps:
- script: echo Hello, world!
  displayName: 'Run a one-line script'

- script: echo Hello, world2!
  displayName: 'Run a one-line script'

- task: Bash@3
  inputs:
    targetType: 'inline'
    script: 'ls -l $(Build.Repository.LocalPath)/dags'

- task: oc-cmd@3
  inputs:
    connectionType: 'OpenShift Connection Service'
    openshiftService: 'OpenshiftCRCIP'
    displayName: 'Run oc get pods'
    cmd: 'get pods'

- task: oc-cmd@3
  inputs:
    connectionType: 'OpenShift Connection Service'
    openshiftService: 'OpenshiftCRCHost'
    displayName: 'Run oc exec to list DAG files in OpenShift POD'
    cmd: 'exec airflow-helm-scheduler-767576bb4c-r2plw -c scheduler -- ls -l /opt/airflow/dags'

- script: |
    echo Add other tasks to build, test, and deploy your project.
    echo See https://aka.ms/yaml
 
  displayName: 'Run a multi-line script'
